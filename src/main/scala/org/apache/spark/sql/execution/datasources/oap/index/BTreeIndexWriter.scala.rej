diff a/src/main/scala/org/apache/spark/sql/execution/datasources/oap/index/BTreeIndexWriter.scala b/src/main/scala/org/apache/spark/sql/execution/datasources/oap/index/BTreeIndexWriter.scala	(rejected hunks)
@@ -24,62 +24,56 @@ import scala.collection.JavaConverters._
 
 import com.google.common.collect.ArrayListMultimap
 import org.apache.hadoop.fs.{FileSystem, Path}
-import org.apache.hadoop.mapreduce.Job
 
-import org.apache.spark.{SparkException, TaskContext}
 import org.apache.spark.rdd.InputFileNameHolder
 import org.apache.spark.sql.catalyst.InternalRow
 import org.apache.spark.sql.catalyst.expressions._
 import org.apache.spark.sql.catalyst.expressions.codegen.GenerateOrdering
-import org.apache.spark.sql.execution.datasources.WriteResult
 import org.apache.spark.sql.execution.datasources.oap.io.IndexFile
 import org.apache.spark.sql.execution.datasources.oap.statistics._
 import org.apache.spark.sql.execution.datasources.oap.utils.{BTreeNode, BTreeUtils}
 import org.apache.spark.sql.types.StructType
-import org.apache.spark.util.Utils
 
 // TODO respect `sparkSession.conf.get(SQLConf.PARTITION_MAX_FILES)`
 private[oap] class BTreeIndexWriter(
-    relation: WriteIndexRelation,
-    job: Job,
     indexColumns: Array[IndexColumn],
     keySchema: StructType,
     indexName: String,
     time: String,
-    isAppend: Boolean) extends IndexWriter(relation, job, isAppend) {
+    isAppend: Boolean) extends IndexWriter {
 
-  override def writeIndexFromRows(
-      taskContext: TaskContext, iterator: Iterator[InternalRow]): Seq[IndexBuildResult] = {
+  override def writeIndexFromRows(description: WriteJobDescription,
+      writer: IndexOutputWriter, iterator: Iterator[InternalRow]): Seq[IndexBuildResult] = {
     var taskReturn: Seq[IndexBuildResult] = Nil
     var writeNewFile = false
-    executorSideSetup(taskContext)
-    val configuration = taskAttemptContext.getConfiguration
+    val configuration = description.serializableHadoopConf.value
     // to get input filename
     if (!iterator.hasNext) return Nil
     // configuration.set(DATASOURCE_OUTPUTPATH, outputPath)
     if (isAppend) {
-      val fs = FileSystem.get(configuration)
+      def isIndexExists(fileName: String): Boolean = {
+        val indexPath =
+          IndexUtils.indexFileFromDataFile(new Path(fileName), indexName, time)
+        val fs = FileSystem.get(configuration)
+        fs.exists(indexPath)
+      }
+
       var nextFile = InputFileNameHolder.getInputFileName().toString
-      var skip = fs.exists(IndexUtils.indexFileFromDataFile(new Path(nextFile), indexName, time))
-      // iterator.next()
+      if(nextFile== null ||  nextFile.isEmpty) return Nil
+      var skip = isIndexExists(nextFile)
+
       while(iterator.hasNext && skip) {
         val cacheFile = nextFile
         nextFile = InputFileNameHolder.getInputFileName().toString
         // avoid calling `fs.exists` for every row
-        skip = cacheFile == nextFile ||
-          fs.exists(IndexUtils.indexFileFromDataFile(new Path(nextFile), indexName, time))
+        skip = cacheFile == nextFile || isIndexExists(nextFile)
         if(skip) iterator.next()
       }
       if (skip) return Nil
     }
+
     val filename = InputFileNameHolder.getInputFileName().toString
-    configuration.set(IndexWriter.INPUT_FILE_NAME, filename)
-    configuration.set(IndexWriter.INDEX_NAME, indexName)
-    configuration.set(IndexWriter.INDEX_TIME, time)
-    // TODO deal with partition
-    // configuration.set(FileOutputFormat.OUTDIR, getWorkPath)
-    var writer = newIndexOutputWriter()
-    writer.initConverter(dataSchema)
+    writer.initIndexInfo(filename, indexName, time)
 
     def buildOrdering(keySchema: StructType): Ordering[InternalRow] = {
       // here i change to use param id to index_id to get datatype in keySchema
@@ -93,33 +87,6 @@ private[oap] class BTreeIndexWriter(
     }
     lazy val ordering = buildOrdering(keySchema)
 
-    def commitTask(): Seq[WriteResult] = {
-      try {
-        var writeResults: Seq[WriteResult] = Nil
-        if (writer != null) {
-          writeResults = writeResults :+ writer.close()
-          writer = null
-        }
-        super.commitTask()
-        writeResults
-      } catch {
-        case cause: Throwable =>
-          // This exception will be handled in `InsertIntoHadoopFsRelation.insert$writeRows`, and
-          // will cause `abortTask()` to be invoked.
-          throw new RuntimeException("Failed to commit task", cause)
-      }
-    }
-
-    def abortTask(): Unit = {
-      try {
-        if (writer != null) {
-          writer.close()
-        }
-      } finally {
-        super.abortTask()
-      }
-    }
-
     def writeTask(): Seq[IndexBuildResult] = {
       val statisticsManager = new StatisticsManager
       statisticsManager.initialize(BTreeIndexType, keySchema, configuration)
@@ -129,7 +96,7 @@ private[oap] class BTreeIndexWriter(
       while (iterator.hasNext && !writeNewFile) {
         val fname = InputFileNameHolder.getInputFileName().toString
         if (fname != filename) {
-          taskReturn = taskReturn ++: writeIndexFromRows(taskContext, iterator)
+          taskReturn = taskReturn ++: writeIndexFromRows(description, writer.copy(), iterator)
           writeNewFile = true
         } else {
           val v = genericProjector(iterator.next()).copy()
@@ -195,20 +162,14 @@ private[oap] class BTreeIndexWriter(
       IndexUtils.writeLong(writer, dataEnd)
       IndexUtils.writeLong(writer, offsetMap.get(uniqueKeysList.getFirst))
 
-      taskReturn :+ IndexBuildResult(filename, cnt, "", new Path(filename).getParent.toString)
-    }
+      // avoid fd leak
+      writer.close
 
-    // If anything below fails, we should abort the task.
-    try {
-      Utils.tryWithSafeFinallyAndFailureCallbacks {
-        val res = writeTask()
-        commitTask()
-        res
-      }(catchBlock = abortTask())
-    } catch {
-      case t: Throwable =>
-        throw new SparkException("Task failed while writing rows", t)
+      taskReturn :+ IndexBuildResult(new Path(filename).getName, cnt, "",
+        new Path(filename).getParent.toString)
     }
+
+    writeTask()
   }
 
   /**
@@ -257,14 +218,13 @@ private[oap] class BTreeIndexWriter(
   /**
    * write file correspond to [[UnsafeIndexNode]]
    */
-  private def writeIndexNode(
-                              tree: BTreeNode,
-                              writer: IndexOutputWriter,
-                              map: java.util.HashMap[InternalRow, Long],
-                              keysList: java.util.LinkedList[InternalRow],
-                              listOffsetFromEnd: Int,
-                              updateOffset: Long,
-                              nextPointer: Long): Long = {
+  private def writeIndexNode(tree: BTreeNode,
+      writer: IndexOutputWriter,
+      map: java.util.HashMap[InternalRow, Long],
+      keysList: java.util.LinkedList[InternalRow],
+      listOffsetFromEnd: Int,
+      updateOffset: Long,
+      nextPointer: Long): Long = {
     var subOffset = 0
     // write road sign count on every node first
     IndexUtils.writeInt(writer, tree.root)
