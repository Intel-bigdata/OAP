diff a/src/main/scala/org/apache/spark/sql/execution/datasources/oap/index/IndexOutputWriter.scala b/src/main/scala/org/apache/spark/sql/execution/datasources/oap/index/IndexOutputWriter.scala	(rejected hunks)
@@ -23,15 +23,26 @@ import org.apache.spark.sql.Row
 import org.apache.spark.sql.execution.datasources._
 
 private[oap] class IndexOutputWriter(
-    bucketId: Option[Int],
-    context: TaskAttemptContext)
+             bucketId: Option[Int],
+             context: TaskAttemptContext)
   extends OutputWriter {
+
+  protected var fileName: String = _
+
+  protected var indexName: String = _
+
+  protected var time: String = _
+
   protected lazy val writer: RecordWriter[Void, Any] = {
     val outputFormat = new OapIndexOutputFormat[Any]()
+    context.getConfiguration.set(IndexWriter.INPUT_FILE_NAME, fileName)
+    context.getConfiguration.set(IndexWriter.INDEX_NAME, indexName)
+    context.getConfiguration.set(IndexWriter.INDEX_TIME, time)
     outputFormat.getRecordWriter(context)
   }
 
   def write(b: Array[Byte]): Unit = write(b, 0, b.length)
+
   def write(b: Array[Byte], off: Int, len: Int): Unit = writer.write(null, b)
 
   def write(i: Int): Unit = writer.write(null, i)
@@ -39,5 +50,16 @@ private[oap] class IndexOutputWriter(
   override def close(): WriteResult = writer.close(context)
 
   override def write(row: Row): Unit = throw new UnsupportedOperationException("don't use this")
+
   // TODO block writeInternal
+
+  def copy(): IndexOutputWriter = new IndexOutputWriter(bucketId, context)
+
+  def initIndexInfo(fileName: String,
+                    indexName: String,
+                    time: String): Unit = {
+    this.fileName = fileName
+    this.indexName = indexName
+    this.time = time
+  }
 }
